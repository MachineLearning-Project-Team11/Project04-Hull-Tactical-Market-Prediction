{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":14348714,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-04T14:55:55.099642Z","iopub.execute_input":"2025-12-04T14:55:55.099927Z","iopub.status.idle":"2025-12-04T14:55:57.520009Z","shell.execute_reply.started":"2025-12-04T14:55:55.099904Z","shell.execute_reply":"2025-12-04T14:55:57.518902Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/hull-tactical-market-prediction/train.csv\n/kaggle/input/hull-tactical-market-prediction/test.csv\n/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/default_inference_server.py\n/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/default_gateway.py\n/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/__init__.py\n/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/templates.py\n/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/base_gateway.py\n/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/relay.py\n/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/kaggle_evaluation.proto\n/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/__init__.py\n/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/generated/kaggle_evaluation_pb2.py\n/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/generated/kaggle_evaluation_pb2_grpc.py\n/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/generated/__init__.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from __future__ import annotations\n\nimport os\nimport sys\nfrom pathlib import Path\nfrom typing import List, Tuple, Sequence\n\nimport numpy as np\nimport pandas as pd\nimport pandas.api.types\nimport polars as pl\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom tqdm import tqdm\nfrom scipy.optimize import minimize\n\n# --- Metric Implementation ---\nMIN_INVESTMENT = 0\nMAX_INVESTMENT = 2\n\nclass ParticipantVisibleError(Exception):\n    pass\n\ndef score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n    \"\"\"\n    Calculates a custom evaluation metric (volatility-adjusted Sharpe ratio).\n    \"\"\"\n    if not pandas.api.types.is_numeric_dtype(submission['prediction']):\n        raise ParticipantVisibleError('Predictions must be numeric')\n\n    solution = solution.copy()\n    solution['position'] = submission['prediction']\n\n    if solution['position'].max() > MAX_INVESTMENT:\n        print(f\"Warning: Position max {solution['position'].max()} > {MAX_INVESTMENT}\")\n    if solution['position'].min() < MIN_INVESTMENT:\n        print(f\"Warning: Position min {solution['position'].min()} < {MIN_INVESTMENT}\")\n\n    solution['strategy_returns'] = solution['risk_free_rate'] * (1 - solution['position']) + solution['position'] * solution['forward_returns']\n\n    # Calculate strategy's Sharpe ratio\n    strategy_excess_returns = solution['strategy_returns'] - solution['risk_free_rate']\n    strategy_excess_cumulative = (1 + strategy_excess_returns).prod()\n    strategy_mean_excess_return = (strategy_excess_cumulative) ** (1 / len(solution)) - 1\n    strategy_std = solution['strategy_returns'].std()\n\n    trading_days_per_yr = 252\n    if strategy_std == 0:\n        return 0.0 \n        \n    sharpe = strategy_mean_excess_return / strategy_std * np.sqrt(trading_days_per_yr)\n    strategy_volatility = float(strategy_std * np.sqrt(trading_days_per_yr) * 100)\n\n    # Calculate market return and volatility\n    market_excess_returns = solution['forward_returns'] - solution['risk_free_rate']\n    market_excess_cumulative = (1 + market_excess_returns).prod()\n    market_mean_excess_return = (market_excess_cumulative) ** (1 / len(solution)) - 1\n    market_std = solution['forward_returns'].std()\n\n    market_volatility = float(market_std * np.sqrt(trading_days_per_yr) * 100)\n\n    if market_volatility == 0:\n        return 0.0\n\n    # Calculate the volatility penalty\n    excess_vol = max(0, strategy_volatility / market_volatility - 1.2) if market_volatility > 0 else 0\n    vol_penalty = 1 + excess_vol\n\n    # Calculate the return penalty\n    return_gap = max(\n        0,\n        (market_mean_excess_return - strategy_mean_excess_return) * 100 * trading_days_per_yr,\n    )\n    return_penalty = 1 + (return_gap**2) / 100\n\n    # Adjust the Sharpe ratio by the volatility and return penalty\n    adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n    return min(float(adjusted_sharpe), 1_000_000)\n\ntry:\n    import lightgbm as lgb\n    LIGHTGBM_AVAILABLE = True\nexcept ImportError:\n    LIGHTGBM_AVAILABLE = False\n\n# --- Configuration & Environment ---\nIS_KAGGLE = Path('/kaggle').exists()\n\nif IS_KAGGLE:\n    INPUT_DIR = Path('/kaggle/input/hull-tactical-market-prediction')\n    sys.path.append(str(INPUT_DIR))\nelse:\n    INPUT_DIR = Path('.')\n    sys.path.append(os.getcwd())\n\nimport kaggle_evaluation.default_inference_server\n\nTRAIN_PATH = INPUT_DIR / 'train.csv'\nTEST_PATH = INPUT_DIR / 'test.csv'\n\nTARGET_COL = 'forward_returns'\nDATE_COL = 'date_id'\nBENCHMARK_COL = 'market_forward_excess_returns'\n\n# --- 1. Data Loading & Feature Engineering ---\n\ndef load_data() -> Tuple[pd.DataFrame, pd.DataFrame]:\n    if not TRAIN_PATH.exists():\n        print(\"Train file not found.\")\n        return pd.DataFrame(), pd.DataFrame()\n        \n    train_df = pd.read_csv(TRAIN_PATH).sort_values(DATE_COL).reset_index(drop=True)\n    \n    if TEST_PATH.exists():\n        test_df = pd.read_csv(TEST_PATH).sort_values(DATE_COL).reset_index(drop=True)\n        print(f\"Loaded {len(test_df)} test rows.\")\n    else:\n        test_df = pd.DataFrame()\n        \n    print(f\"Loaded {len(train_df)} train rows.\")\n    return train_df, test_df\n\ndef engineer_features(df: pd.DataFrame, numeric_cols: Sequence[str], show_progress: bool = False) -> Tuple[pd.DataFrame, List[str]]:\n    feats = df.copy()\n    created_cols: List[str] = []\n    new_features = []\n    \n    # Lags and Rolling stats\n    LAG_WINDOWS = (1, 3, 5, 10, 21, 63) # 영업일\n    ROLL_WINDOWS = (5, 21, 63)\n    \n    iterator = tqdm(numeric_cols, desc=\"Engineering Features\", disable=not show_progress)\n    \n    for col in iterator:\n        if col not in feats.columns:\n            continue\n        if not pd.api.types.is_numeric_dtype(feats[col]):\n            continue\n            \n        # Lags\n        for lag in LAG_WINDOWS:\n            lag_col = f'{col}_lag_{lag}'\n            new_features.append(feats[col].shift(lag).rename(lag_col))\n            created_cols.append(lag_col)\n            \n        # Rolling stats\n        for window in ROLL_WINDOWS:\n            roll_mean = f'{col}_roll_mean_{window}'\n            roll_std = f'{col}_roll_std_{window}'\n            \n            # Calculate rolling stats\n            rolling = feats[col].rolling(window)\n            new_features.append(rolling.mean().shift(1).rename(roll_mean))\n            new_features.append(rolling.std(ddof=0).shift(1).rename(roll_std))\n            \n            created_cols.extend([roll_mean, roll_std])\n            \n    if new_features:\n        feats = pd.concat([feats] + new_features, axis=1)\n        \n    base_cols = [c for c in numeric_cols if c in feats.columns]\n    feature_cols = base_cols + created_cols\n    return feats, feature_cols\n\nclass FeatureGenerator:\n    def __init__(self, numeric_cols: List[str]):\n        self.numeric_cols = numeric_cols\n        self.history = pd.DataFrame()\n        \n    def fit(self, df: pd.DataFrame):\n        # Store last 100 rows to cover max rolling window (63) + buffer\n        if not df.empty:\n            self.history = df[self.numeric_cols].iloc[-100:].copy()\n        \n    def transform(self, new_df: pd.DataFrame) -> pd.DataFrame:\n        # Append new data to history\n        # Ensure we only keep relevant columns to save memory/time\n        new_data_subset = new_df[self.numeric_cols]\n        combined = pd.concat([self.history, new_data_subset], axis=0)\n        \n        # Calculate features on the combined window\n        # We reuse the batch function\n        feats, _ = engineer_features(combined, self.numeric_cols)\n        \n        # Extract features for the new rows only\n        # The new rows are at the end\n        new_feats = feats.iloc[-len(new_df):].copy()\n        \n        # Update history (keep last 100)\n        self.history = combined.iloc[-100:]\n        \n        return new_feats\n\n# --- 2. Model Development ---\n\ndef train_model(train_df: pd.DataFrame, feature_cols: List[str], model_type: str = 'lgbm'):\n    X = train_df[feature_cols].fillna(0)\n    y = train_df[TARGET_COL]\n    \n    print(f\"Training {model_type} on {len(X)} rows with {len(feature_cols)} features...\")\n    \n    if model_type == 'lgbm' and LIGHTGBM_AVAILABLE:\n        model = lgb.LGBMRegressor(n_estimators=200, learning_rate=0.05, num_leaves=31, random_state=42, verbose=-1)\n        model.fit(X, y)\n        print(\"LGBM training completed.\")\n    else:\n        # Fallback to RF\n        model = RandomForestRegressor(n_estimators=100, max_depth=10, n_jobs=-1, random_state=42)\n        model.fit(X, y)\n        print(\"RandomForest training completed.\")\n        \n    return model\n\n# --- 3. Global State & API ---\n\n# Global variables for the API\nmodel = None\nfeature_generator = None\nfeature_cols = []\ntarget_mean = 0.0\ntarget_std = 1.0\nPREDICT_COUNTER = 0\n# Optimization params\nOPT_SCALE = 0.5\nOPT_BIAS = 1.0\n\ndef predict(test: pl.DataFrame) -> float:\n    \"\"\"\n    API Inference Function.\n    Receives a batch of test data (Polars), returns a single float prediction.\n    \"\"\"\n    global model, feature_generator, feature_cols, target_mean, target_std, PREDICT_COUNTER, OPT_SCALE, OPT_BIAS\n    \n    PREDICT_COUNTER += 1\n    if PREDICT_COUNTER % 100 == 0 or PREDICT_COUNTER <= 5:\n        print(f\"Processed {PREDICT_COUNTER} batches...\", flush=True)\n    \n    try:\n        # Convert to Pandas\n        test_df = test.to_pandas()\n        \n        # Generate Features\n        # This updates history and returns features for the current batch\n        X_test_full = feature_generator.transform(test_df)\n        \n        # Select feature columns and fill NaNs\n        X_test = X_test_full[feature_cols].fillna(0)\n        \n        # Predict\n        # We assume single row prediction\n        preds = model.predict(X_test)\n        pred_val = preds[0]\n        \n        # Post-process (Z-score -> Weight)\n        # Use optimized parameters\n        z_score = (pred_val - target_mean) / (target_std + 1e-8)\n        \n        # weight = bias + scale * z_score\n        weight = OPT_BIAS + OPT_SCALE * z_score\n        weight = np.clip(weight, 0.0, 2.0)\n        \n        return float(weight)\n    except Exception as e:\n        # Fallback\n        return 1.0\n\n# --- 4. Main Execution ---\n\ndef main():\n    global model, feature_generator, feature_cols, target_mean, target_std\n    \n    print(\"Loading data...\")\n    train_df, _ = load_data()\n    \n    if train_df.empty:\n        print(\"No training data found. Exiting.\")\n        return\n\n    # Identify numeric columns for features\n    # We must only use columns that are present in BOTH train and test (or at least expected in test)\n    # 'risk_free_rate' might be in train but not test?\n    # Let's check intersection with test columns if available, or just handle missing cols in generator.\n    \n    EXCLUDE_COLS = {TARGET_COL, 'weight', 'row_id', DATE_COL, BENCHMARK_COL}\n    numeric_cols = [c for c in train_df.columns if c not in EXCLUDE_COLS and pd.api.types.is_numeric_dtype(train_df[c])]\n    \n    # Filter numeric_cols to those likely available in test\n    # If test_df is loaded, use intersection\n    if TEST_PATH.exists():\n        test_df_check = pd.read_csv(TEST_PATH, nrows=1)\n        numeric_cols = [c for c in numeric_cols if c in test_df_check.columns]\n    \n    print(f\"Selected {len(numeric_cols)} numeric columns for features.\")\n    \n    print(\"Engineering features for training...\")\n    train_feat, generated_cols = engineer_features(train_df, numeric_cols, show_progress=True)\n    \n    # Define final feature list\n    # Ensure unique columns\n    feature_cols = list(dict.fromkeys([c for c in numeric_cols if c in train_feat.columns] + generated_cols))\n    \n    # Clean training data\n    train_feat_clean = train_feat.dropna(subset=feature_cols + [TARGET_COL]).reset_index(drop=True)\n    \n    # Calculate stats for z-scoring\n    target_mean = train_feat_clean[TARGET_COL].mean()\n    target_std = train_feat_clean[TARGET_COL].std()\n    \n    # Train Model\n    model = train_model(train_feat_clean, feature_cols, model_type='lgbm')\n    \n    # Calculate In-Sample Score\n    print(\"Calculating In-Sample Score & Optimizing Parameters...\")\n    global OPT_SCALE, OPT_BIAS\n    \n    try:\n        # Predict on training set\n        X_train = train_feat_clean[feature_cols].fillna(0)\n        train_preds = model.predict(X_train)\n        \n        # Post-process\n        train_z = (train_preds - target_mean) / (target_std + 1e-8)\n        \n        # Optimization\n        solution = train_feat_clean[[TARGET_COL, 'risk_free_rate']].copy()\n        \n        def objective(params):\n            sc, bi = params\n            weights = np.clip(bi + sc * train_z, 0.0, 2.0)\n            submission = pd.DataFrame({'prediction': weights})\n            try:\n                return -score(solution, submission, DATE_COL)\n            except:\n                return 0.0\n                \n        x0 = [0.5, 1.0]\n        res = minimize(objective, x0, method='Nelder-Mead', tol=1e-4, options={'maxiter': 100})\n        \n        OPT_SCALE, OPT_BIAS = res.x\n        print(f\"Optimization result: {res.message}\")\n        print(f\"Best Score: {-res.fun:.4f}\")\n        print(f\"Params: scale={OPT_SCALE:.4f}, bias={OPT_BIAS:.4f}\")\n        \n        # Final check\n        train_weights = np.clip(OPT_BIAS + OPT_SCALE * train_z, 0.0, 2.0)\n        submission = pd.DataFrame({'prediction': train_weights})\n        train_score = score(solution, submission, DATE_COL)\n        print(f\"Final In-Sample Volatility-Adjusted Sharpe: {train_score:.4f}\")\n        \n    except Exception as e:\n        print(f\"Could not calculate in-sample score: {e}\")\n    \n    # Initialize Feature Generator with Training Data\n    print(\"Initializing Feature Generator...\")\n    feature_generator = FeatureGenerator(numeric_cols)\n    feature_generator.fit(train_df)\n    \n    # Setup Inference Server\n    print(\"Setting up Inference Server...\")\n    inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n\n    if IS_KAGGLE:\n        if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n            print(\"Starting inference server (Competition Mode)...\")\n            inference_server.serve()\n        else:\n            print(\"Running local gateway on Kaggle (Interactive Mode)...\")\n            inference_server.run_local_gateway(\n                (str(INPUT_DIR),)\n            )\n    else:\n        print(\"Running local gateway (Local Mode)...\")\n        # This will generate submission.parquet in the current directory\n        try:\n            inference_server.run_local_gateway(\n                (str(INPUT_DIR),)\n            )\n        except Exception as e:\n            print(f\"Local gateway error: {e}\")\n            # Fallback: Manual generation for local testing if gateway fails\n            if TEST_PATH.exists():\n                print(\"Fallback: Generating submission.parquet manually...\")\n                test_df = pd.read_csv(TEST_PATH).sort_values(DATE_COL).reset_index(drop=True)\n                test_pl = pl.from_pandas(test_df)\n                \n                # Reset generator to end of train\n                feature_generator.fit(train_df)\n                \n                # Manual loop for local file generation\n                weights = []\n                ids = []\n                for i in tqdm(range(len(test_pl)), desc=\"Manual Prediction Loop\"):\n                    row_pl = test_pl[i]\n                    w = predict(row_pl)\n                    weights.append(w)\n                    \n                    row_pd = row_pl.to_pandas()\n                    id_val = row_pd['row_id'].iloc[0] if 'row_id' in row_pd.columns else row_pd['date_id'].iloc[0]\n                    ids.append(id_val)\n                \n                submission_pl = pl.DataFrame({'date_id': ids, 'prediction': weights})\n                submission_pl.write_parquet('submission.parquet')\n                print(\"Success: submission.parquet generated manually.\")\n        \n    if Path('submission.parquet').exists():\n        print(\"Success: submission.parquet generated.\")\n    else:\n        print(\"Warning: submission.parquet not found.\")\n\nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T14:56:35.927902Z","iopub.execute_input":"2025-12-04T14:56:35.928348Z","iopub.status.idle":"2025-12-04T14:57:04.691918Z","shell.execute_reply.started":"2025-12-04T14:56:35.928312Z","shell.execute_reply":"2025-12-04T14:57:04.690719Z"}},"outputs":[{"name":"stdout","text":"Loading data...\nLoaded 10 test rows.\nLoaded 9021 train rows.\nSelected 94 numeric columns for features.\nEngineering features for training...\n","output_type":"stream"},{"name":"stderr","text":"Engineering Features: 100%|██████████| 94/94 [00:00<00:00, 184.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training lgbm on 1989 rows with 1222 features...\nLGBM training completed.\nCalculating In-Sample Score & Optimizing Parameters...\nOptimization result: Optimization terminated successfully.\nBest Score: 10.0548\nParams: scale=3432.4234, bias=96.9130\nFinal In-Sample Volatility-Adjusted Sharpe: 10.0548\nInitializing Feature Generator...\nSetting up Inference Server...\nRunning local gateway on Kaggle (Interactive Mode)...\nProcessed 1 batches...\nProcessed 2 batches...\nProcessed 3 batches...\nProcessed 4 batches...\nProcessed 5 batches...\nSuccess: submission.parquet generated.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}