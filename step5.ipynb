{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a501725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Sequence, Optional, Any\n",
    "from collections import deque\n",
    "import math\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "import polars as pl\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.optimize import minimize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- [Configuration] ---\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['axes.unicode_minus'] = False \n",
    "\n",
    "MIN_INVESTMENT = 0\n",
    "MAX_INVESTMENT = 2 \n",
    "USE_SAM = True  \n",
    "EMA_DECAY = 0.999\n",
    "REPLAY_BUFFER_SIZE = 252 \n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_SEEDS = 2           # [NEW] Ensemble 개수 (속도와 성능의 타협점)\n",
    "TARGET_VOLATILITY = 0.15 # [NEW] 목표 연간 변동성 (15%)\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "IS_KAGGLE = Path('/kaggle').exists()\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    INPUT_DIR = Path('/kaggle/input/hull-tactical-market-prediction')\n",
    "    sys.path.append(str(INPUT_DIR))\n",
    "else:\n",
    "    INPUT_DIR = Path('.')\n",
    "    sys.path.append(os.getcwd())\n",
    "\n",
    "try:\n",
    "    import kaggle_evaluation.default_inference_server\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "TRAIN_PATH = INPUT_DIR / 'train.csv'\n",
    "TEST_PATH = INPUT_DIR / 'test.csv'\n",
    "TARGET_COL = 'forward_returns'\n",
    "DATE_COL = 'date_id'\n",
    "BENCHMARK_COL = 'market_forward_excess_returns'\n",
    "\n",
    "# --- [Utils] ---\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "class SlidingWindowBuffer:\n",
    "    def __init__(self, capacity=252):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state: np.ndarray, target: float):\n",
    "        self.buffer.append((state, target))\n",
    "    \n",
    "    def sample(self, batch_size: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        curr_len = len(self.buffer)\n",
    "        if curr_len == 0:\n",
    "            return np.array([]), np.array([])\n",
    "        sample_size = min(curr_len, batch_size)\n",
    "        # Prioritize recent data but keep some randomness if needed. \n",
    "        # Here we simple take latest window for Online Learning stability.\n",
    "        batch = list(self.buffer)[-sample_size:]\n",
    "        states, targets = zip(*batch)\n",
    "        return np.array(states), np.array(targets)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# --- [Optimizer: SAM] ---\n",
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        self.defaults.update(self.base_optimizer.defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                self.state[p][\"old_p\"] = p.data.clone()\n",
    "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
    "                p.add_(e_w)\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.data = self.state[p][\"old_p\"]\n",
    "        self.base_optimizer.step()\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm\n",
    "    \n",
    "    def zero_grad(self, set_to_none: bool = False):\n",
    "        self.base_optimizer.zero_grad(set_to_none)\n",
    "\n",
    "# --- [Data Loading & Evaluation] ---\n",
    "def load_data() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    if not TRAIN_PATH.exists():\n",
    "        print('Train file not found.')\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "    train_df = pd.read_csv(TRAIN_PATH).sort_values(DATE_COL).reset_index(drop=True)\n",
    "    if TEST_PATH.exists():\n",
    "        test_df = pd.read_csv(TEST_PATH).sort_values(DATE_COL).reset_index(drop=True)\n",
    "    else:\n",
    "        test_df = pd.DataFrame()\n",
    "    return train_df, test_df\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str = DATE_COL) -> float:\n",
    "    # (기존 score 함수와 동일하므로 생략 없이 유지)\n",
    "    if 'prediction' not in submission.columns:\n",
    "        raise ParticipantVisibleError('Submission must contain a prediction column')\n",
    "    \n",
    "    sol = solution.copy()\n",
    "    sol['position'] = submission['prediction']\n",
    "    \n",
    "    if 'risk_free_rate' not in sol.columns:\n",
    "        sol['risk_free_rate'] = 0.0\n",
    "\n",
    "    sol['strategy_returns'] = sol['risk_free_rate'] * (1 - sol['position']) + sol['position'] * sol['forward_returns']\n",
    "    strategy_excess_returns = sol['strategy_returns'] - sol['risk_free_rate']\n",
    "    \n",
    "    trading_days_per_yr = 252\n",
    "    \n",
    "    mean_excess = strategy_excess_returns.mean()\n",
    "    std_excess = strategy_excess_returns.std()\n",
    "    \n",
    "    if std_excess == 0:\n",
    "        return 0.0\n",
    "\n",
    "    sharpe = (mean_excess / std_excess) * np.sqrt(trading_days_per_yr)\n",
    "    \n",
    "    market_std = sol['forward_returns'].std()\n",
    "    strategy_volatility = std_excess * np.sqrt(trading_days_per_yr) * 100\n",
    "    market_volatility = market_std * np.sqrt(trading_days_per_yr) * 100\n",
    "    \n",
    "    if market_volatility == 0: \n",
    "        return 0.0\n",
    "\n",
    "    excess_vol = max(0.0, strategy_volatility / market_volatility - 1.2)\n",
    "    vol_penalty = 1 + excess_vol\n",
    "    \n",
    "    strat_ann_ret = mean_excess * trading_days_per_yr * 100\n",
    "    \n",
    "    if BENCHMARK_COL in sol.columns:\n",
    "        market_mean_excess = sol[BENCHMARK_COL].mean()\n",
    "    else:\n",
    "        market_mean_excess = (sol['forward_returns'] - sol['risk_free_rate']).mean()\n",
    "        \n",
    "    market_ann_ret = market_mean_excess * trading_days_per_yr * 100\n",
    "    \n",
    "    return_gap = max(0.0, market_ann_ret - strat_ann_ret)\n",
    "    return_penalty = 1 + (return_gap ** 2) / 100\n",
    "    \n",
    "    return float(sharpe / (vol_penalty * return_penalty))\n",
    "\n",
    "# --- [Feature Engineering] ---\n",
    "# [Improvement 1] Advanced Alpha Factors added\n",
    "def engineer_features(df: pd.DataFrame, numeric_cols: Sequence[str], show_progress: bool = False) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    feats = df.copy()\n",
    "    new_features = []\n",
    "    created_cols = []\n",
    "\n",
    "    lag_windows = (1, 2, 3, 5, 10, 21)\n",
    "    roll_windows = (5, 10, 21, 63)\n",
    "\n",
    "    def calculate_rsi(series, period=14):\n",
    "        delta = series.diff().fillna(0)\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "        rs = gain / (loss + 1e-8)\n",
    "        return 100 - (100 / (1 + rs))\n",
    "\n",
    "    iterator = tqdm(numeric_cols, desc='Engineering Features', disable=not show_progress)\n",
    "\n",
    "    for col in iterator:\n",
    "        if col not in feats.columns: continue\n",
    "        if not pd.api.types.is_numeric_dtype(feats[col]): continue\n",
    "\n",
    "        # 1. Basic Lags\n",
    "        for lag in lag_windows:\n",
    "            lag_col = f'{col}_lag_{lag}'\n",
    "            new_features.append(feats[col].shift(lag).rename(lag_col))\n",
    "            created_cols.append(lag_col)\n",
    "\n",
    "        # 2. Rolling Stats\n",
    "        for window in roll_windows:\n",
    "            rolling = feats[col].rolling(window)\n",
    "            \n",
    "            # Mean & Std\n",
    "            roll_mean = f'{col}_roll_mean_{window}'\n",
    "            roll_std = f'{col}_roll_std_{window}'\n",
    "            new_features.append(rolling.mean().shift(1).rename(roll_mean))\n",
    "            new_features.append(rolling.std(ddof=0).shift(1).rename(roll_std))\n",
    "            created_cols.extend([roll_mean, roll_std])\n",
    "\n",
    "            # [NEW] Skewness & Kurtosis (Higher Moments)\n",
    "            if window >= 21:\n",
    "                roll_skew = f'{col}_skew_{window}'\n",
    "                roll_kurt = f'{col}_kurt_{window}'\n",
    "                new_features.append(rolling.skew().shift(1).rename(roll_skew))\n",
    "                new_features.append(rolling.kurt().shift(1).rename(roll_kurt))\n",
    "                created_cols.extend([roll_skew, roll_kurt])\n",
    "\n",
    "        # 3. RSI\n",
    "        rsi_col_name = f'{col}_rsi_14'\n",
    "        rsi_series = calculate_rsi(feats[col], 14).rename(rsi_col_name)\n",
    "        new_features.append(rsi_series)\n",
    "        created_cols.append(rsi_col_name)\n",
    "\n",
    "        # 4. Volatility Ratio\n",
    "        vol_ratio_name = f'{col}_vol_ratio'\n",
    "        roll_std_5 = feats[col].rolling(5).std()\n",
    "        roll_std_21 = feats[col].rolling(21).std()\n",
    "        vol_ratio = (roll_std_5 / (roll_std_21 + 1e-8)).rename(vol_ratio_name)\n",
    "        new_features.append(vol_ratio)\n",
    "        created_cols.append(vol_ratio_name)\n",
    "        \n",
    "        # [NEW] 5. MACD (Moving Average Convergence Divergence)\n",
    "        ema_12 = feats[col].ewm(span=12, adjust=False).mean()\n",
    "        ema_26 = feats[col].ewm(span=26, adjust=False).mean()\n",
    "        macd = (ema_12 - ema_26).rename(f'{col}_macd')\n",
    "        new_features.append(macd)\n",
    "        created_cols.append(f'{col}_macd')\n",
    "\n",
    "        # [NEW] 6. Bollinger Bands Width\n",
    "        bb_mean = feats[col].rolling(20).mean()\n",
    "        bb_std = feats[col].rolling(20).std()\n",
    "        bb_upper = bb_mean + 2 * bb_std\n",
    "        bb_lower = bb_mean - 2 * bb_std\n",
    "        bb_width = ((bb_upper - bb_lower) / (bb_mean + 1e-8)).rename(f'{col}_bb_width')\n",
    "        new_features.append(bb_width)\n",
    "        created_cols.append(f'{col}_bb_width')\n",
    "\n",
    "    if new_features:\n",
    "        feats = pd.concat([feats] + new_features, axis=1)\n",
    "\n",
    "    feats = feats.copy() \n",
    "    feats = feats.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    base_cols = [c for c in numeric_cols if c in feats.columns]\n",
    "    feature_cols = base_cols + created_cols\n",
    "    return feats, feature_cols\n",
    "\n",
    "class FeatureGenerator:\n",
    "    def __init__(self, numeric_cols: List[str]):\n",
    "        self.numeric_cols = numeric_cols\n",
    "        self.history = pd.DataFrame()\n",
    "\n",
    "    def fit(self, df: pd.DataFrame):\n",
    "        if not df.empty:\n",
    "            # Need larger history for MACD/Skew calculations\n",
    "            self.history = df[self.numeric_cols].iloc[-300:].copy() \n",
    "\n",
    "    def transform(self, new_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        new_data_subset = new_df[self.numeric_cols]\n",
    "        combined = pd.concat([self.history, new_data_subset], axis=0)\n",
    "        feats, _ = engineer_features(combined, self.numeric_cols)\n",
    "        new_feats = feats.iloc[-len(new_df):].copy()\n",
    "        self.history = combined.iloc[-300:]\n",
    "        return new_feats\n",
    "\n",
    "# --- [Model Architecture] ---\n",
    "# [Improvement 5] SE-Block for Feature Attention\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, features) -> Unsqueeze to (batch, features, 1) for pooling simulation if needed, \n",
    "        # but since input is already 1D per sample, we treat 'features' as 'channels'.\n",
    "        b, c = x.size()\n",
    "        y = self.fc(x)\n",
    "        return x * y\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class MultiScaleEnsemble(nn.Module):\n",
    "    def __init__(self, feature_names: List[str], hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.feature_names = feature_names\n",
    "        \n",
    "        # Feature grouping logic\n",
    "        self.short_indices = []\n",
    "        self.mid_indices = []\n",
    "        self.long_indices = []\n",
    "        self.base_indices = []\n",
    "\n",
    "        for i, name in enumerate(feature_names):\n",
    "            if '63' in name or 'macd' in name or 'kurt' in name or 'skew' in name:\n",
    "                self.long_indices.append(i)\n",
    "            elif '21' in name or '10' in name or 'bb_width' in name:\n",
    "                self.mid_indices.append(i)\n",
    "            elif 'lag' in name or 'rsi' in name or 'vol' in name:\n",
    "                self.short_indices.append(i)\n",
    "            else:\n",
    "                self.base_indices.append(i)\n",
    "        \n",
    "        if not self.short_indices: self.short_indices = self.base_indices\n",
    "        if not self.mid_indices: self.mid_indices = self.base_indices\n",
    "        if not self.long_indices: self.long_indices = self.base_indices\n",
    "        \n",
    "        self.short_input_idx = list(set(self.base_indices + self.short_indices))\n",
    "        \n",
    "        def make_branch(input_dim):\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                SEBlock(hidden_dim, reduction=8), # [NEW] Apply SE-Block\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.SiLU(),\n",
    "                ResidualBlock(hidden_dim),\n",
    "                ResidualBlock(hidden_dim),\n",
    "                nn.Linear(hidden_dim, 1)\n",
    "            )\n",
    "\n",
    "        self.net_short = make_branch(len(self.short_input_idx))\n",
    "        self.net_mid = make_branch(len(self.mid_indices))\n",
    "        self.net_long = make_branch(len(self.long_indices))\n",
    "        \n",
    "        self.gating_net = nn.Sequential(\n",
    "            nn.Linear(len(feature_names), 32), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 3),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_short = x[:, self.short_input_idx]\n",
    "        x_mid = x[:, self.mid_indices]\n",
    "        x_long = x[:, self.long_indices]\n",
    "        \n",
    "        out_short = self.net_short(x_short)\n",
    "        out_mid = self.net_mid(x_mid)\n",
    "        out_long = self.net_long(x_long)\n",
    "        \n",
    "        weights = self.gating_net(x)\n",
    "        \n",
    "        out = (out_short * weights[:, 0:1]) + \\\n",
    "              (out_mid * weights[:, 1:2]) + \\\n",
    "              (out_long * weights[:, 2:3])\n",
    "        \n",
    "        return torch.tanh(out), weights\n",
    "\n",
    "# --- [Loss Function] ---\n",
    "# [Improvement 2] Differentiable Sharpe Loss\n",
    "class SharpeHybridLoss(nn.Module):\n",
    "    def __init__(self, target_return=0.0, alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.target_return = target_return\n",
    "        self.alpha = alpha # Weight for Sharpe Component\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        # 1. MSE Component (Stabilizer)\n",
    "        mse_loss = self.mse(preds, targets)\n",
    "        \n",
    "        # 2. Sharpe Component (Optimizer)\n",
    "        strategy_returns = preds * targets\n",
    "        expected_return = torch.mean(strategy_returns)\n",
    "        volatility = torch.std(strategy_returns) + 1e-8\n",
    "        \n",
    "        # Negate Sharpe because we minimize loss\n",
    "        sharpe_loss = -1.0 * (expected_return / volatility)\n",
    "        \n",
    "        return (1 - self.alpha) * mse_loss + self.alpha * sharpe_loss\n",
    "\n",
    "# --- [Ensemble Trainer] ---\n",
    "# [Improvement 3] Seed Ensemble Logic\n",
    "class EnsembleTrainerWrapper:\n",
    "    def __init__(self, feature_names: List[str], hidden_dim: int = 128, num_seeds: int = NUM_SEEDS):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.models = nn.ModuleList()\n",
    "        self.ema_models = nn.ModuleList()\n",
    "        self.optimizers = []\n",
    "        self.num_seeds = num_seeds\n",
    "        self.loss_fn = SharpeHybridLoss(alpha=0.1) # Small alpha for Sharpe to avoid instability in small batches\n",
    "\n",
    "        for i in range(num_seeds):\n",
    "            # Seed initialization handled implicitly by loop if we didn't fix seed globally,\n",
    "            # but usually it's better to re-seed or let random init differ.\n",
    "            # Here pytorch linear layers init randomly.\n",
    "            net = MultiScaleEnsemble(feature_names, hidden_dim).to(self.device)\n",
    "            ema_net = MultiScaleEnsemble(feature_names, hidden_dim).to(self.device)\n",
    "            ema_net.load_state_dict(net.state_dict())\n",
    "            ema_net.eval()\n",
    "            \n",
    "            self.models.append(net)\n",
    "            self.ema_models.append(ema_net)\n",
    "            \n",
    "            base_optim = optim.AdamW\n",
    "            if USE_SAM:\n",
    "                opt = SAM(net.parameters(), base_optim, lr=LEARNING_RATE, rho=0.05, weight_decay=1e-5)\n",
    "            else:\n",
    "                opt = base_optim(net.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "            self.optimizers.append(opt)\n",
    "            \n",
    "        self.ema_decay = EMA_DECAY\n",
    "\n",
    "    def update_ema(self):\n",
    "        with torch.no_grad():\n",
    "            for i in range(self.num_seeds):\n",
    "                for param_q, param_k in zip(self.models[i].parameters(), self.ema_models[i].parameters()):\n",
    "                    param_k.data = param_k.data * self.ema_decay + param_q.data * (1. - self.ema_decay)\n",
    "\n",
    "    def fit_batch(self, X: np.ndarray, y: np.ndarray, epochs: int = 1):\n",
    "        if len(X) == 0: return 0.0\n",
    "        \n",
    "        X_tensor = torch.from_numpy(X).float().to(self.device)\n",
    "        y_tensor = torch.from_numpy(y).float().view(-1, 1).to(self.device)\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for i in range(self.num_seeds):\n",
    "            self.models[i].train()\n",
    "            \n",
    "            for _ in range(epochs):\n",
    "                if USE_SAM:\n",
    "                    # First Step\n",
    "                    preds, _ = self.models[i](X_tensor)\n",
    "                    loss = self.loss_fn(preds, y_tensor)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.models[i].parameters(), 1.0)\n",
    "                    self.optimizers[i].first_step(zero_grad=True)\n",
    "                    \n",
    "                    # Second Step\n",
    "                    preds_2, _ = self.models[i](X_tensor) # [수정]\n",
    "                    self.loss_fn(preds_2, y_tensor).backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.models[i].parameters(), 1.0)\n",
    "                    self.optimizers[i].second_step(zero_grad=True)\n",
    "                else:\n",
    "                    self.optimizers[i].zero_grad()\n",
    "                    preds, _ = self.models[i](X_tensor)\n",
    "                    loss = self.loss_fn(preds, y_tensor)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.models[i].parameters(), 1.0)\n",
    "                    self.optimizers[i].step()\n",
    "            \n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        self.update_ema()\n",
    "        return total_loss / (epochs * self.num_seeds)\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        X_tensor = torch.from_numpy(X).float().to(self.device)\n",
    "        preds_accum = torch.zeros(X.shape[0], 1).to(self.device)\n",
    "        weights_accum = torch.zeros(X.shape[0], 3).to(self.device) # 가중치 누적용\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(self.num_seeds):\n",
    "                self.ema_models[i].eval()\n",
    "                # [수정] 예측값과 가중치를 모두 받음\n",
    "                p, w = self.ema_models[i](X_tensor)\n",
    "                preds_accum += p\n",
    "                weights_accum += w\n",
    "                \n",
    "        avg_preds = preds_accum / self.num_seeds\n",
    "        avg_weights = weights_accum / self.num_seeds # 가중치 평균\n",
    "        \n",
    "        # 예측값과 가중치 모두 반환\n",
    "        return avg_preds.cpu().numpy().reshape(-1), avg_weights.cpu().numpy()\n",
    "\n",
    "# --- [Online Training & Optimization] ---\n",
    "class OnlineTrainer:\n",
    "    def __init__(self, model: EnsembleTrainerWrapper, buffer_size=REPLAY_BUFFER_SIZE, batch_size=BATCH_SIZE):\n",
    "        self.model = model\n",
    "        self.buffer = SlidingWindowBuffer(capacity=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.pending_features: deque[np.ndarray] = deque()\n",
    "        self.pending_raw_preds: deque[float] = deque()\n",
    "        self.update_count = 0\n",
    "\n",
    "    def queue_example(self, feature_vector: np.ndarray, raw_pred: float):\n",
    "        self.pending_features.append(feature_vector.astype(np.float32))\n",
    "        self.pending_raw_preds.append(float(raw_pred))\n",
    "\n",
    "    def apply_feedback(self, actual_target: float) -> Optional[float]:\n",
    "        if not self.pending_features:\n",
    "            return None\n",
    "\n",
    "        feat_vec = self.pending_features.popleft()\n",
    "        _ = self.pending_raw_preds.popleft()\n",
    "        \n",
    "        self.buffer.push(feat_vec, actual_target)\n",
    "        \n",
    "        loss = None\n",
    "        if len(self.buffer) >= self.batch_size:\n",
    "            states, targets = self.buffer.sample(self.batch_size)\n",
    "            loss = self.model.fit_batch(states, targets, epochs=1)\n",
    "            self.update_count += 1\n",
    "        \n",
    "        return loss\n",
    "\n",
    "class OnlineOptimizer:\n",
    "    def __init__(self, initial_scale=1.0, initial_bias=1.0, window_size=252):\n",
    "        self.scale = initial_scale\n",
    "        self.bias = initial_bias\n",
    "        self.window_size = window_size\n",
    "\n",
    "        self.history_preds: deque[float] = deque(maxlen=window_size)\n",
    "        self.history_actuals: deque[float] = deque(maxlen=window_size)\n",
    "        self.history_rfr: deque[float] = deque(maxlen=window_size)\n",
    "\n",
    "        self.last_raw_prediction: float | None = None\n",
    "\n",
    "    def update(self, lagged_return: float, lagged_rfr: float):\n",
    "        if self.last_raw_prediction is not None and np.isfinite(lagged_return):\n",
    "            self.history_preds.append(self.last_raw_prediction)\n",
    "            self.history_actuals.append(lagged_return)\n",
    "            self.history_rfr.append(lagged_rfr)\n",
    "\n",
    "            if len(self.history_preds) >= 100 and len(self.history_preds) % 50 == 0:\n",
    "                self._optimize()\n",
    "\n",
    "    def _optimize(self):\n",
    "        preds = np.array(self.history_preds)\n",
    "        actuals = np.array(self.history_actuals)\n",
    "        rfrs = np.array(self.history_rfr)\n",
    "\n",
    "        p_mean = np.mean(preds)\n",
    "        p_std = np.std(preds) + 1e-8\n",
    "        z_scores = (preds - p_mean) / p_std\n",
    "\n",
    "        def objective(params):\n",
    "            sc, bi = params\n",
    "            weights = np.clip(bi + sc * z_scores, MIN_INVESTMENT, MAX_INVESTMENT)\n",
    "            excess_returns = weights * (actuals - rfrs)\n",
    "            \n",
    "            mean_ret = np.mean(excess_returns)\n",
    "            std_ret = np.std(excess_returns)\n",
    "            \n",
    "            if std_ret < 1e-7: return 100.0\n",
    "            sharpe = mean_ret / std_ret\n",
    "            return -sharpe\n",
    "\n",
    "        x0 = [self.scale, self.bias]\n",
    "        bounds = [(0.0, 5.0), (0.0, 2.0)]\n",
    "\n",
    "        try:\n",
    "            res = minimize(objective, x0, method='L-BFGS-B', bounds=bounds, tol=1e-4)\n",
    "            if res.success:\n",
    "                alpha = 0.05 \n",
    "                self.scale = alpha * res.x[0] + (1 - alpha) * self.scale\n",
    "                self.bias = alpha * res.x[1] + (1 - alpha) * self.bias\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def get_params(self) -> Tuple[float, float]:\n",
    "        return self.scale, self.bias\n",
    "\n",
    "    def set_last_prediction(self, pred: float):\n",
    "        self.last_raw_prediction = float(pred)\n",
    "    \n",
    "    # [Improvement 4] Volatility Scaling Logic\n",
    "    def get_vol_scaler(self) -> float:\n",
    "        if len(self.history_actuals) < 21:\n",
    "            return 1.0\n",
    "        \n",
    "        # Calculate recent realized volatility (21 days)\n",
    "        recent_returns = list(self.history_actuals)[-21:]\n",
    "        current_vol = np.std(recent_returns) * np.sqrt(252)\n",
    "        \n",
    "        if current_vol == 0: return 1.0\n",
    "        \n",
    "        # Target Volatility Scaling\n",
    "        scaler = TARGET_VOLATILITY / (current_vol + 1e-8)\n",
    "        \n",
    "        # Clip to prevent extreme scaling (e.g. 0.5x to 1.5x)\n",
    "        return np.clip(scaler, 0.5, 1.5)\n",
    "\n",
    "# --- [Visualization] ---\n",
    "def evaluate_and_plot_performance(val_df: pd.DataFrame):\n",
    "    df = val_df.copy()\n",
    "    \n",
    "    df['strategy_return'] = df['prediction'] * df['forward_returns']\n",
    "    \n",
    "    if BENCHMARK_COL in df.columns:\n",
    "        df['market_return'] = df[BENCHMARK_COL]\n",
    "    else:\n",
    "        df['market_return'] = df['forward_returns']\n",
    "\n",
    "    df['cum_strategy'] = (1 + df['strategy_return']).cumprod() - 1\n",
    "    df['cum_market'] = (1 + df['market_return']).cumprod() - 1\n",
    "\n",
    "    running_max = df['cum_strategy'].cummax()\n",
    "    df['drawdown'] = (df['cum_strategy'] - running_max) / (running_max + 1)\n",
    "    max_dd = df['drawdown'].min()\n",
    "    \n",
    "    final_strat_ret = df['cum_strategy'].iloc[-1]\n",
    "    final_mkt_ret = df['cum_market'].iloc[-1]\n",
    "    \n",
    "    print(f\"\\n{'='*20} [Performance Report] {'='*20}\")\n",
    "    print(f\"Strategy Final Return : {final_strat_ret*100:.2f}%\")\n",
    "    print(f\"Market Final Return   : {final_mkt_ret*100:.2f}%\")\n",
    "    print(f\"Max Drawdown          : {max_dd*100:.2f}%\")\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 1, figsize=(15, 12), sharex=True)\n",
    "    \n",
    "    axes[0].plot(df['cum_strategy'], label='Strategy (Ensemble)', color='blue', linewidth=2)\n",
    "    axes[0].plot(df['cum_market'], label='Market Benchmark', color='gray', linestyle='--', alpha=0.7)\n",
    "    axes[0].set_title('Cumulative Returns: Model vs Market', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylabel('Cumulative Return')\n",
    "    axes[0].legend(loc='upper left')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    window = 63\n",
    "    roll_mean = df['strategy_return'].rolling(window).mean()\n",
    "    roll_std = df['strategy_return'].rolling(window).std()\n",
    "    roll_sharpe = (roll_mean / (roll_std + 1e-9)) * np.sqrt(252)\n",
    "    \n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(roll_sharpe, label=f'{window}-Day Rolling Sharpe', color='green', alpha=0.8)\n",
    "    ax2.axhline(0, color='black', linewidth=0.8, linestyle='--')\n",
    "    ax2.set_ylabel('Rolling Sharpe Ratio', color='green')\n",
    "    ax2.set_title(f'Risk Analysis: {window}-Day Rolling Sharpe & Volatility', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(loc='upper left')\n",
    "    \n",
    "    ax2_r = ax2.twinx()\n",
    "    annualized_vol = roll_std * np.sqrt(252)\n",
    "    ax2_r.plot(annualized_vol, label=f'{window}-Day Rolling Volatility', color='orange', alpha=0.5, linestyle=':')\n",
    "    ax2_r.set_ylabel('Annualized Volatility', color='orange')\n",
    "    ax2_r.legend(loc='upper right')\n",
    "\n",
    "    axes[2].fill_between(df.index, df['drawdown'], 0, color='red', alpha=0.3, label='Drawdown')\n",
    "    axes[2].plot(df['drawdown'], color='red', linewidth=1)\n",
    "    axes[2].set_title('Underwater Plot (Drawdown Profile)', fontsize=14, fontweight='bold')\n",
    "    axes[2].set_ylabel('Drawdown %')\n",
    "    axes[2].set_xlabel('Time Steps (Days)')\n",
    "    axes[2].legend(loc='lower left')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_investment_weights(val_df: pd.DataFrame):\n",
    "    df = val_df.copy()\n",
    "    weights = df['prediction']\n",
    "    \n",
    "    mean_w = weights.mean()\n",
    "    std_w = weights.std()\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 6))\n",
    "    plt.plot(df.index, weights, label='Daily Weight', color='steelblue', alpha=0.6, linewidth=1)\n",
    "    plt.plot(df.index, weights.rolling(21).mean(), label='21-Day Avg Weight', color='orange', linewidth=2)\n",
    "    plt.title('Daily Investment Weight History', fontsize=14, fontweight='bold')\n",
    "    plt.axhline(mean_w, color='red', linestyle='--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# --- [Simulation Loops] ---\n",
    "def run_online_simulation_loop(\n",
    "    df: pd.DataFrame,\n",
    "    model: EnsembleTrainerWrapper,\n",
    "    trainer: OnlineTrainer,\n",
    "    optimizer: OnlineOptimizer,\n",
    "    scaler: RobustScaler,\n",
    "    feature_cols: List[str],\n",
    "    desc: str = \"Simulating\",\n",
    "    update_model: bool = True\n",
    ") -> Tuple[List[float], List[np.ndarray]]:\n",
    "    \n",
    "    preds_list = []\n",
    "    gating_weights_list = []\n",
    "    \n",
    "    for i in tqdm(range(len(df)), desc=desc):\n",
    "        row = df.iloc[i:i+1]\n",
    "        \n",
    "        raw_features = row[feature_cols].fillna(0).to_numpy(dtype=np.float64)\n",
    "        scaled_features = scaler.transform(raw_features)\n",
    "        \n",
    "        preds, w = model.predict(scaled_features)\n",
    "        raw_pred = float(preds[0])\n",
    "        gating_weights_list.append(w[0])\n",
    "        \n",
    "        trainer.queue_example(scaled_features[0], raw_pred)\n",
    "        optimizer.set_last_prediction(raw_pred)\n",
    "        \n",
    "        opt_scale, opt_bias = optimizer.get_params()\n",
    "        \n",
    "        # [Improvement 4] Apply Volatility Scaling\n",
    "        vol_scalar = optimizer.get_vol_scaler()\n",
    "        \n",
    "        weight = opt_bias + opt_scale * raw_pred\n",
    "        weight_adjusted = weight * vol_scalar\n",
    "        \n",
    "        final_weight = float(np.clip(weight_adjusted, MIN_INVESTMENT, MAX_INVESTMENT))\n",
    "        \n",
    "        preds_list.append(final_weight)\n",
    "        \n",
    "        if update_model:\n",
    "            actual = float(row[TARGET_COL].iloc[0])\n",
    "            rfr = float(row['risk_free_rate'].iloc[0]) if 'risk_free_rate' in row.columns else 0.0\n",
    "            \n",
    "            trainer.apply_feedback(actual)\n",
    "            optimizer.update(actual, rfr)\n",
    "            \n",
    "    return preds_list, gating_weights_list\n",
    "\n",
    "def plot_gating_weights_with_market(df: pd.DataFrame, weights_list: List[np.ndarray]):\n",
    "    \"\"\"\n",
    "    df: 검증 데이터프레임\n",
    "    weights_list: 시뮬레이션에서 저장한 가중치 리스트 (N x 3)\n",
    "    \"\"\"\n",
    "    weights_np = np.array(weights_list) # (N, 3)\n",
    "    dates = df.index\n",
    "    \n",
    "    # 시장 누적 수익률 계산\n",
    "    market_ret = df[BENCHMARK_COL] if BENCHMARK_COL in df.columns else df['forward_returns']\n",
    "    cum_market = (1 + market_ret).cumprod() - 1\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "    # --- 1. Gating Weights (Line Chart) ---\n",
    "    # 각각을 별도의 선으로 그립니다.\n",
    "    # Short: 파랑, Mid: 주황, Long: 초록 (기존 색상 테마 유지)\n",
    "    ax1.plot(dates, weights_np[:, 0], label='Short-Term (Blue)', color='royalblue', linewidth=1.5, alpha=0.9)\n",
    "    ax1.plot(dates, weights_np[:, 1], label='Mid-Term (Orange)', color='darkorange', linewidth=1.5, alpha=0.9)\n",
    "    ax1.plot(dates, weights_np[:, 2], label='Long-Term (Green)', color='forestgreen', linewidth=1.5, alpha=0.9)\n",
    "    \n",
    "    ax1.set_ylabel('Gating Weight (0.0 ~ 1.0)', fontsize=12)\n",
    "    ax1.set_ylim(-0.05, 1.05) # 여백을 살짝 둠\n",
    "    ax1.legend(loc='upper left', ncol=3)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.title('Gating Weights Trends (Line Chart) vs Market Condition', fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def run_validation_and_score(\n",
    "    df: pd.DataFrame, \n",
    "    feature_cols: List[str], \n",
    "    split_ratio: float = 0.8\n",
    "):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"[Full Walk-Forward Validation Start]\")\n",
    "    print(f\"Ensemble Seeds: {NUM_SEEDS}\")\n",
    "    \n",
    "    split_idx = int(len(df) * split_ratio)\n",
    "    train_data = df.iloc[:split_idx].copy()\n",
    "    val_data = df.iloc[split_idx:].reset_index(drop=True).copy()\n",
    "    \n",
    "    print(f\"Train samples: {len(train_data)}, Validation samples: {len(val_data)}\")\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    train_X = train_data[feature_cols].fillna(0).to_numpy(dtype=np.float64)\n",
    "    scaler.fit(train_X)\n",
    "    \n",
    "    model = EnsembleTrainerWrapper(feature_cols)\n",
    "    trainer = OnlineTrainer(model, buffer_size=REPLAY_BUFFER_SIZE, batch_size=BATCH_SIZE)\n",
    "    optimizer = OnlineOptimizer(initial_scale=1.0, initial_bias=1.0)\n",
    "    \n",
    "    print(\"\\nPhase 1: Walking through TRAIN data (Learning)...\")\n",
    "    _ = run_online_simulation_loop(\n",
    "        train_data, model, trainer, optimizer, scaler, feature_cols,\n",
    "        desc=\"Train Walk-Forward\", update_model=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nPhase 2: Walking through VALIDATION data (Evaluating)...\")\n",
    "    val_preds, val_weights = run_online_simulation_loop(\n",
    "        val_data, model, trainer, optimizer, scaler, feature_cols,\n",
    "        desc=\"Val Walk-Forward\", update_model=True\n",
    "    )\n",
    "    \n",
    "    val_data['prediction'] = val_preds\n",
    "    if 'risk_free_rate' not in val_data.columns:\n",
    "        val_data['risk_free_rate'] = 0.0\n",
    "        \n",
    "    sharpe_score = score(val_data, val_data)\n",
    "    \n",
    "    evaluate_and_plot_performance(val_data)\n",
    "    plot_gating_weights_with_market(val_data, val_weights)\n",
    "    plot_investment_weights(val_data)\n",
    "\n",
    "    print(f\"\\n[Validation Result]\")\n",
    "    print(f\"Validation Modified Sharpe Ratio: {sharpe_score:.5f}\")\n",
    "    returns = val_data['prediction'] * val_data[TARGET_COL]\n",
    "    print(f\"Mean Return: {returns.mean():.5f}, Std: {returns.std():.5f}\")\n",
    "    print(f\"{'='*40}\\n\")\n",
    "    \n",
    "    return sharpe_score\n",
    "\n",
    "def walkforward_simulation(\n",
    "    train_feat_clean: pd.DataFrame,\n",
    "    feature_cols: List[str],\n",
    "):\n",
    "    print(\"\\nStarting Final Production Walk-Forward on FULL Data...\")\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    all_X = train_feat_clean[feature_cols].fillna(0).to_numpy(dtype=np.float64)\n",
    "    scaler.fit(all_X)\n",
    "    \n",
    "    model = EnsembleTrainerWrapper(feature_cols)\n",
    "    trainer = OnlineTrainer(model, buffer_size=REPLAY_BUFFER_SIZE, batch_size=BATCH_SIZE)\n",
    "    optimizer = OnlineOptimizer(initial_scale=1.0, initial_bias=1.0)\n",
    "    \n",
    "    t_mean = train_feat_clean[TARGET_COL].mean()\n",
    "    t_std = train_feat_clean[TARGET_COL].std()\n",
    "\n",
    "    _ = run_online_simulation_loop(\n",
    "        train_feat_clean, model, trainer, optimizer, scaler, feature_cols,\n",
    "        desc='Final Full Simulation', update_model=True\n",
    "    )\n",
    "        \n",
    "    return scaler, model, trainer, optimizer, t_mean, t_std\n",
    "\n",
    "# --- [Inference Globals] ---\n",
    "model: Optional[Any] = None\n",
    "feature_generator: Optional[FeatureGenerator] = None\n",
    "feature_cols: List[str] = []\n",
    "target_mean = 0.0\n",
    "target_std = 1.0\n",
    "optimizer: OnlineOptimizer | None = None\n",
    "online_trainer: Optional[OnlineTrainer] = None\n",
    "scaler: Optional[RobustScaler] = None\n",
    "PREDICT_COUNTER = 0\n",
    "\n",
    "def predict(test: pl.DataFrame) -> float:\n",
    "    global model, feature_generator, feature_cols, target_mean, target_std, optimizer, online_trainer, scaler, PREDICT_COUNTER\n",
    "\n",
    "    if optimizer is None or model is None:\n",
    "        return 1.0\n",
    "\n",
    "    if 'lagged_forward_returns' in test.columns and 'lagged_risk_free_rate' in test.columns:\n",
    "        lagged_ret = test['lagged_forward_returns'][0]\n",
    "        lagged_rfr = test['lagged_risk_free_rate'][0]\n",
    "        \n",
    "        if lagged_ret is not None and lagged_rfr is not None:\n",
    "            loss = online_trainer.apply_feedback(float(lagged_ret))\n",
    "            optimizer.update(float(lagged_ret), float(lagged_rfr))\n",
    "\n",
    "    try:\n",
    "        test_df = test.to_pandas()\n",
    "        X_test_full = feature_generator.transform(test_df)\n",
    "        X_test = X_test_full[feature_cols].fillna(0)\n",
    "        X_np = X_test.to_numpy(dtype=np.float64)\n",
    "        \n",
    "        X_scaled = scaler.transform(X_np)\n",
    "        preds = model.predict(X_scaled)\n",
    "        raw_pred = float(preds[0])\n",
    "\n",
    "        online_trainer.queue_example(X_scaled[0], raw_pred)\n",
    "        optimizer.set_last_prediction(raw_pred)\n",
    "\n",
    "        opt_scale, opt_bias = optimizer.get_params()\n",
    "        vol_scalar = optimizer.get_vol_scaler()\n",
    "        \n",
    "        weight = opt_bias + opt_scale * raw_pred\n",
    "        weight_adjusted = weight * vol_scalar\n",
    "        \n",
    "        final_weight = float(np.clip(weight_adjusted, MIN_INVESTMENT, MAX_INVESTMENT))\n",
    "        \n",
    "        PREDICT_COUNTER += 1\n",
    "        return final_weight\n",
    "        \n",
    "    except Exception as err:\n",
    "        print(f'Predict error: {err}')\n",
    "        return 1.0\n",
    "\n",
    "def manual_inference_loop(train_df: pd.DataFrame):\n",
    "    if feature_generator is None: return\n",
    "    if not TEST_PATH.exists():\n",
    "        print('Test file not found.')\n",
    "        return\n",
    "\n",
    "    test_df = pd.read_csv(TEST_PATH).sort_values(DATE_COL).reset_index(drop=True)\n",
    "    test_pl = pl.from_pandas(test_df)\n",
    "\n",
    "    weights = []\n",
    "    ids = []\n",
    "\n",
    "    for i in tqdm(range(len(test_pl)), desc='Manual Predict Loop'):\n",
    "        row_pl = test_pl[i]\n",
    "        weight = predict(row_pl)\n",
    "        weights.append(weight)\n",
    "        row_pd = row_pl.to_pandas()\n",
    "        if 'row_id' in row_pd.columns:\n",
    "            ids.append(row_pd['row_id'].iloc[0])\n",
    "        else:\n",
    "            ids.append(row_pd[DATE_COL].iloc[0])\n",
    "\n",
    "    submission_pl = pl.DataFrame({'row_id': ids, 'prediction': weights})\n",
    "    submission_pl.write_csv('submission.csv')\n",
    "    print(f'submission.csv generated ({len(submission_pl)} rows).')\n",
    "\n",
    "def main():\n",
    "    global model, feature_generator, feature_cols, target_mean, target_std, optimizer, online_trainer, scaler\n",
    "\n",
    "    train_df, _ = load_data()\n",
    "    if train_df.empty: return\n",
    "\n",
    "    exclude_cols = {TARGET_COL, 'weight', 'row_id', DATE_COL, BENCHMARK_COL}\n",
    "    numeric_cols = [c for c in train_df.columns if c not in exclude_cols and pd.api.types.is_numeric_dtype(train_df[c])]\n",
    "\n",
    "    if TEST_PATH.exists():\n",
    "        test_sample = pd.read_csv(TEST_PATH, nrows=1)\n",
    "        numeric_cols = [c for c in numeric_cols if c in test_sample.columns]\n",
    "\n",
    "    train_feat, generated_cols = engineer_features(train_df, numeric_cols, show_progress=True)\n",
    "    feature_cols = list(dict.fromkeys([c for c in numeric_cols if c in train_feat.columns] + generated_cols))\n",
    "\n",
    "    train_feat_clean = train_feat.dropna(subset=feature_cols + [TARGET_COL]).reset_index(drop=True)\n",
    "\n",
    "    # 90% train, 10% validation\n",
    "    run_validation_and_score(train_feat_clean, feature_cols, split_ratio=0.90899949723479135243841126194067)\n",
    "    \n",
    "    # Final Training on Full Data\n",
    "    scaler, model, online_trainer, optimizer, target_mean, target_std = walkforward_simulation(\n",
    "        train_feat_clean, feature_cols\n",
    "    )\n",
    "\n",
    "    feature_generator = FeatureGenerator(numeric_cols)\n",
    "    feature_generator.fit(train_df)\n",
    "\n",
    "    print(f'Ready for inference: Scale={optimizer.scale:.3f}, Bias={optimizer.bias:.3f}')\n",
    "\n",
    "    if IS_KAGGLE:\n",
    "        import kaggle_evaluation.default_inference_server\n",
    "        inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n",
    "        if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "            inference_server.serve()\n",
    "        else:\n",
    "            inference_server.run_local_gateway((str(INPUT_DIR),))\n",
    "    else:\n",
    "        if not Path('submission.parquet').exists():\n",
    "            manual_inference_loop(train_df)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
